# 01. 아파치 스파크란

- 통합 컴퓨팅 엔진
- 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합
- 지원 언어 : 파이썬, 자바, 스칼라, R
- 단일 노트북 환경에서부터 수천 대의 서버로 구성된 클러스터까지 다양한 환경에서 실행 가능
- 제공하는 컴포넌트 및 라이브럴
	- 구조적 스트리밍
	- 고급 분석
	- 라이브러리 및 에코 시스템
	- 구조적 API : Dataset, DataFrame, SQL
	- 저수준 API : RDD, 분산형 변수

## 1.1. 아파치 스파크의 철학

### 통합
- 스파크 핵심 목표 : **'빅데이터 애플리케이션 개발에 필요한 통합 플랫폼을 제공하자'**
- 스파크의 통합 특성을 이용하면 **기존의 데이터 분석 작업을 더 쉽고 효율적으로 수행**할 수 있음
- 소프트웨어 영역의 여러 통합 플랫폼과 유사한 방식을 제공
### 컴퓨팅 엔진
- 스파크는 통합이라는 관점을 중시하면서 **기능의 범위를 컴퓨팅 엔진으로 제한**
- **저장소 시스템의 데이터 연산 역할만 수행**
	- 영구 저장소 역할은 수행하지 않아 다양한 저장소 지원 :
	  애저 스토리지, 아마존 S3, 아파치 하둡, 아파치 카산드라, 아파치 카프카 등
- 데이터 저장 위치에 상관없이 처리에 집중 가능
- 사용자 API는 서로 다른 저장소 시스템을 매우 유사하게 볼 수 있음
### 라이브러리
- 엔진에서 제공하는 표준 라이브러리
	- 스파크 SQL
	- MLlib
	- 스파크 스트리밍
	- 구조적 스트리밍
	- GraphX
- 오픈소스 커뮤니티에서 서드파티 패키지 형태로 제공하는 다양한 외부 라이브러리
	- [spark-packages.org](https://spark-packages.org/) 확인

## 1.2. 스파크의 등장 배경
- 데이터 수집 비용의 급격한 감소
- 데이터 크기 증가 : 단일 머신으로 처리 불가. 클러스터에서 분산 처리 필요
- 기존 모델의 한계 : 처리 속도 느림 + 복잡한 분석/반복 작업에 부적합
➡️ 이런 문제를 해결하기 위해 아파치 스파크 탄생

## 1.3. 스파크의 역사
- 2009년 UC버클리 대학교에서 스파크 연구 프로젝트로 시작
- 'Spark: Cluster Computing with Working Sets' 논문을 통해 처음 알려짐
- 초기 배치 애플리케이션만 지원 ➡️ 대화형 데이터 분석이나 비정형 쿼리 같은 기능 제공
- 2014년 스파크 1.0 버전, 2016년 스파크 2.0 버전 공개
	- 1.0 이전 : 함수형 연산 관점에서 API 정의
	- 1.0 : 구조화된 데이터를 기반으로 동작하는 신규 API인 스파크 SQL 추가
	- 2.0 : DataFrame, 머신러닝 파이프라인, 구조적 스트리밍 등 강력한 구조체 기반의 신규 API 추가


## (+) 스파크의 기존 빅데이터 플랫폼과 차별화

|                 | **Apache Spark**                                     | **Hadoop MapReduce (기존 플랫폼)**           |
| --------------- | ---------------------------------------------------- | --------------------------------------- |
| 연산 모델           | In-memory 연산(RDD, DAG 기반)                            | 디스크 기반                                  |
| 속도              | 빠름(최대 수십 배)                                          | 상대적으로 느림 (디스크 I/O 병목)                   |
| 데이터 처리 구조       | Lazy Evaluation, DAG 최적화                             | 단계별 연산 진행(Map/Reduce로 고정)               |
| 복잡한 연산 처리       | 반복 연산, 머신러닝, 그래프 처리에 적합                              | 반복 연산에 비효율적 (매번 디스크 I/O 발생)             |
| API 지원          | 고수준 API (SQL, MLlib, GraphX, Structured Streaming 등) | 저수준 API (Java 기반 Map/Reduce 코드)         |
| 메모리 활용          | 클러스터 메모리 활용 극대화                                      | 대부분 디스크 기반 저장 중심                        |
| 실시간 처리          | Structured Streaming 제공                              | 실시간 처리 불가 또는 외부 시스템 필요 (Storm, Flink 등) |
| 재사용성/재실행        | RDD/DF 캐싱 가능, 재시작 빠름                                 | 매번 재실행, 중간 결과 저장 필요                     |
| 코드 간결성          | 간결하고 직관적인 코드 가능 (Scala, Python, SQL)                 | 장황한 Map/Reduce 코드                       |
| Fault Tolerance | Lineage로 자동 복구 (RDD 기준)                              | 중간 결과 재시작, 체크 포인트 필요                    |



> 본 게시글은 [스파크 완벽 가이드](https://product.kyobobook.co.kr/detail/S000001810100) 도서를 참고하여 작성되었습니다.
>
> 상세한 내용이 궁금하시다면 책을 읽어보실 것을 추천해 드립니다.